{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNGImKn2CS1jOaMWGL0pYxp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArthAgrawal/NLP_Concepts/blob/main/NLP_Concepts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzhgL_QR_S_t"
      },
      "outputs": [],
      "source": [
        "# !pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# nltk.download('punkt')"
      ],
      "metadata": {
        "id": "qiQ7kevxAXl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello everyone. Welcome to GeeksforGeeks. You are studying NLP article.\"\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "print(sentences)"
      ],
      "metadata": {
        "id": "zFZr3ZMeBh7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello everyone. Welcome to GeeksforGeeks. You are studying NLP article.\"\n",
        "words = nltk.word_tokenize(text)\n",
        "print(words)"
      ],
      "metadata": {
        "id": "_ZunslfTBjfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# words = nltk.word_tokenize(sentences)\n",
        "# print(words)"
      ],
      "metadata": {
        "id": "UUQqpd6lCBdc"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above code doesn't work because nltk.word_tokenize() expects a string only NOT a list"
      ],
      "metadata": {
        "id": "YH1vdJy6DkDO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"नमस्ते, मेरा नाम अर्थ है\"\n",
        "words = nltk.word_tokenize(text)\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltkNSWSPDWT_",
        "outputId": "39b9cd8a-c435-4ab5-a665-88c526945e17"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['नमस्ते', ',', 'मेरा', 'नाम', 'अर्थ', 'है']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install spacy"
      ],
      "metadata": {
        "id": "UdKQEU8HEVX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another way for tokenization is using a library called Spacy\n"
      ],
      "metadata": {
        "id": "dhcToI7yFbZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "doc = nlp(\"GeeksforGeeks is a one stop learning destination for geeks.\")\n",
        "\n",
        "for token in doc:\n",
        "\tprint(token)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6AEKMNhFNvU",
        "outputId": "3b721a33-b7b4-42f2-e1ac-8c839af51eb4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GeeksforGeeks\n",
            "is\n",
            "a\n",
            "one\n",
            "stop\n",
            "learning\n",
            "destination\n",
            "for\n",
            "geeks\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello everyone. Welcome to GeeksforGeeks. You are studying NLP article.\"\n",
        "words = nltk.word_tokenize(text)\n",
        "\n",
        "for i in range(len(words)):\n",
        "    words[i] = words[i].lower()\n",
        "\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpAtlfdfFSpI",
        "outputId": "f29a31a7-8b79-4882-d770-00f471caaa60"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', 'everyone', '.', 'welcome', 'to', 'geeksforgeeks', '.', 'you', 'are', 'studying', 'nlp', 'article', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# nltk.download('stopwords')        # This downloads stopwords from 16 different languages(Hindi not included)\n",
        "\n",
        "\n",
        "text=\"The film's development began when Marvel Studios received a loan from Merrill Lynch in April 2005. After the success of the film Iron Man in May 2008, Marvel announced that The Avengers would be released in July 2011 and would bring together Stark (Downey), Rogers (Evans), Banner (at the time portrayed by Edward Norton),[b] and Thor (Hemsworth) from Marvel's previous films. With the signing of Johansson as Romanoff in March 2009, Renner as Barton in June 2010, and Ruffalo replacing Norton as Banner in July 2010, the film was pushed back for a 2012 release. Whedon was brought on board in April 2010 and rewrote the original screenplay by Zak Penn. Production began in April 2011 in Albuquerque, New Mexico, before moving to Cleveland, Ohio in August and New York City in September. The film has more than 2,200 visual effects shots.\"\n",
        "\n",
        "\n",
        "stop_words = set(stopwords.words('english'))      # Gives set of stopwords of English\n",
        "word_tokens = word_tokenize(text)\n",
        "\n",
        "for i in range(len(word_tokens)):                 # Making all words lowercase\n",
        "    word_tokens[i] = word_tokens[i].lower()\n",
        "\n",
        "\n",
        "filtered_sentence = [w for w in word_tokens if not w in stop_words]     # Using list comprehension to append all those words into list which are not stopwords\n",
        "\n",
        "print(filtered_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiEfwrMkIon8",
        "outputId": "5bfdd243-bfd7-49c0-8c3b-a1606ee9ac6c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['film', \"'s\", 'development', 'began', 'marvel', 'studios', 'received', 'loan', 'merrill', 'lynch', 'april', '2005', '.', 'success', 'film', 'iron', 'man', 'may', '2008', ',', 'marvel', 'announced', 'avengers', 'would', 'released', 'july', '2011', 'would', 'bring', 'together', 'stark', '(', 'downey', ')', ',', 'rogers', '(', 'evans', ')', ',', 'banner', '(', 'time', 'portrayed', 'edward', 'norton', ')', ',', '[', 'b', ']', 'thor', '(', 'hemsworth', ')', 'marvel', \"'s\", 'previous', 'films', '.', 'signing', 'johansson', 'romanoff', 'march', '2009', ',', 'renner', 'barton', 'june', '2010', ',', 'ruffalo', 'replacing', 'norton', 'banner', 'july', '2010', ',', 'film', 'pushed', 'back', '2012', 'release', '.', 'whedon', 'brought', 'board', 'april', '2010', 'rewrote', 'original', 'screenplay', 'zak', 'penn', '.', 'production', 'began', 'april', '2011', 'albuquerque', ',', 'new', 'mexico', ',', 'moving', 'cleveland', ',', 'ohio', 'august', 'new', 'york', 'city', 'september', '.', 'film', '2,200', 'visual', 'effects', 'shots', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('spanish'))\n",
        "print(stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkEsWmhgQs5B",
        "outputId": "295ce35d-6347-4072-ee42-35097a26cabe"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'siente', 'habremos', 'hubieras', 'estaríamos', 'tenían', 'ese', 'pero', 'seréis', 'he', 'fuesen', 'nosotras', 'suyas', 'serías', 'estéis', 'por', 'tuya', 'tuve', 'fuerais', 'fuésemos', 'hubiesen', 'era', 'tengas', 'su', 'están', 'habréis', 'hayamos', 'estaría', 'les', 'han', 'estarías', 'tuyas', 'hubisteis', 'uno', 'estuvieron', 'tuvieras', 'habríamos', 'nos', 'ellos', 'ni', 'fuiste', 'habidas', 'de', 'habéis', 'tienes', 'ti', 'hubo', 'suya', 'porque', 'hasta', 'tuviesen', 'quien', 'vuestras', 'tenida', 'todos', 'a', 'tened', 'mi', 'vuestros', 'más', 'se', 'vuestra', 'hay', 'sería', 'que', 'vuestro', 'hubieron', 'también', 'hubiésemos', 'otras', 'tenéis', 'estados', 'es', 'tuviese', 'estará', 'sus', 'habidos', 'para', 'tengo', 'estar', 'tengan', 'tendrías', 'está', 'durante', 'ya', 'e', 'seríamos', 'yo', 'habrán', 'una', 'estuvieseis', 'seré', 'somos', 'tuviste', 'vosotros', 'tuyo', 'sentid', 'tuvieseis', 'tuvieron', 'muy', 'nada', 'habrá', 'qué', 'habré', 'hubimos', 'soy', 'mucho', 'tengamos', 'nosotros', 'estuve', 'estuvierais', 'donde', 'estaremos', 'tuvo', 'habrían', 'los', 'nuestro', 'estamos', 'tendré', 'teníais', 'tendríais', 'sin', 'sea', 'estaríais', 'hayas', 'fueran', 'eran', 'estaré', 'sean', 'estaréis', 'míos', 'habría', 'habida', 'os', 'estoy', 'ante', 'seremos', 'otros', 'tendremos', 'algunas', 'tuviéramos', 'estas', 'estén', 'habrás', 'estarás', 'estuvieses', 'fuese', 'serían', 'estarán', 'habíais', 'tanto', 'esas', 'eres', 'eso', 'hubieses', 'tiene', 'tuvieran', 'vosotras', 'estés', 'o', 'estuvo', 'estemos', 'tendríamos', 'estuviera', 'tenidos', 'tuviésemos', 'seáis', 'cual', 'fueses', 'mis', 'te', 'sois', 'algunos', 'me', 'cuando', 'todo', 'no', 'esos', 'un', 'él', 'mí', 'entre', 'contra', 'fuera', 'tuvieses', 'tenido', 'estuvieras', 'seríais', 'tuvierais', 'hemos', 'habrías', 'nuestros', 'estarían', 'hubieseis', 'seamos', 'fueseis', 'estaban', 'tu', 'estás', 'la', 'esa', 'tenía', 'habiendo', 'sentido', 'estuviésemos', 'teniendo', 'estado', 'desde', 'tenidas', 'estuvisteis', 'esté', 'mías', 'hayáis', 'suyos', 'tenemos', 'fui', 'estando', 'hubiese', 'tuvimos', 'hubieran', 'has', 'estuviesen', 'estábamos', 'estáis', 'tendrán', 'tendría', 'en', 'había', 'estuvieran', 'el', 'seas', 'habías', 'esto', 'estos', 'este', 'erais', 'sí', 'otro', 'estabais', 'tenga', 'como', 'hubierais', 'estuviste', 'éramos', 'tendréis', 'algo', 'nuestra', 'las', 'hubiera', 'lo', 'serás', 'poco', 'eras', 'sobre', 'habido', 'tú', 'nuestras', 'fueron', 'tuvisteis', 'otra', 'tuyos', 'son', 'tendrían', 'ellas', 'sentida', 'estaba', 'estabas', 'tenías', 'tus', 'hubiste', 'quienes', 'suyo', 'estuvimos', 'haya', 'y', 'sentidas', 'le', 'habíamos', 'hayan', 'estad', 'tienen', 'del', 'habían', 'con', 'tendrás', 'teníamos', 'muchos', 'ella', 'estada', 'sentidos', 'fueras', 'esta', 'habríais', 'antes', 'ha', 'sintiendo', 'fuéramos', 'estuviese', 'mío', 'hube', 'tuviera', 'fuimos', 'fuisteis', 'estadas', 'hubiéramos', 'tengáis', 'serán', 'unos', 'estuviéramos', 'mía', 'será', 'fue', 'tendrá', 'al'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another easy way to remove stopwords is by using the gensim library as follows\n"
      ],
      "metadata": {
        "id": "sPmdf50yT-hs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "\n",
        "# Another sample text\n",
        "new_text = \"The majestic mountains provide a breathtaking view.\"\n",
        "\n",
        "# Remove stopwords using Gensim\n",
        "new_filtered_text = remove_stopwords(new_text)\n",
        "\n",
        "print(\"Original Text:\", new_text)\n",
        "print(\"Text after Stopword Removal:\", new_filtered_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvJWhsjCTi5s",
        "outputId": "9f58a7fb-56b0-41de-bcd0-f4256db8493b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: The majestic mountains provide a breathtaking view.\n",
            "Text after Stopword Removal: The majestic mountains provide breathtaking view.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also customise the stopwords according to our need using simple set operations on the stopwords set. It is shown as follows"
      ],
      "metadata": {
        "id": "x_Y6sDpVUpi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Get the default NLTK stopwords list for English\n",
        "nltk_stopwords = set(stopwords.words('english'))\n",
        "\n",
        "# Define your custom stopwords\n",
        "custom_stopwords = set(['the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'being', 'been', 'do', 'does', 'did', 'have', 'has', 'had', 'having'])\n",
        "\n",
        "# Update the NLTK stopwords list with your custom stopwords\n",
        "customized_stopwords = nltk_stopwords.union(custom_stopwords)\n",
        "\n",
        "# Tokenize a sample sentence\n",
        "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
        "tokens = word_tokenize(sentence)\n",
        "\n",
        "# Filter out the custom stopwords\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in customized_stopwords]\n",
        "\n",
        "print(\"Original Sentence:\", sentence)\n",
        "print(\"Filtered Sentence:\", ' '.join(filtered_tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKA7GShmU1I0",
        "outputId": "2db02dd0-5af5-42f7-a7cf-fcbc7fcbbdf0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sentence: The quick brown fox jumps over the lazy dog\n",
            "Filtered Sentence: quick brown fox jumps lazy dog\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# create an object of class PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "print(porter.stem(\"play\"))\n",
        "print(porter.stem(\"playing\"))\n",
        "print(porter.stem(\"plays\"))\n",
        "print(porter.stem(\"played\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUJUGSvvU1-W",
        "outputId": "e80b4c56-c173-4a7e-bc7b-5bc14a6a7205"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "play\n",
            "play\n",
            "play\n",
            "play\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(porter.stem(\"finally\"))\n",
        "print(porter.stem(\"finalized\"))\n",
        "print(porter.stem(\"finalizer\"))\n",
        "print(porter.stem(\"finalization\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CctL9JuOnB2I",
        "outputId": "f69b6d15-6888-4465-ab51-d70719c23032"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "final\n",
            "final\n",
            "final\n",
            "final\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another Stemmer we can use is Snowball Stemmer.\n",
        "It is faster and more accurate than porter stemmer"
      ],
      "metadata": {
        "id": "0gcZiYBp6Pas"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, SnowballStemmer\n",
        "\n",
        "# Initialize the stemmers\n",
        "porter_stemmer = PorterStemmer()\n",
        "snowball_stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "# Sample words\n",
        "words = [\"running\", \"jumps\", \"easily\", \"fairly\", \"arguably\"]\n",
        "\n",
        "# Apply Porter Stemmer\n",
        "porter_stems = [porter_stemmer.stem(word) for word in words]\n",
        "\n",
        "# Apply Snowball Stemmer\n",
        "snowball_stems = [snowball_stemmer.stem(word) for word in words]\n",
        "\n",
        "print(\"Original Words:\", words)\n",
        "print(\"Porter Stemmer Results:\", porter_stems)\n",
        "print(\"Snowball Stemmer Results:\", snowball_stems)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwBpVcoSnLE3",
        "outputId": "846b5b7f-896f-4782-e6a0-783d47c64a43"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Words: ['running', 'jumps', 'easily', 'fairly', 'arguably']\n",
            "Porter Stemmer Results: ['run', 'jump', 'easili', 'fairli', 'arguabl']\n",
            "Snowball Stemmer Results: ['run', 'jump', 'easili', 'fair', 'arguabl']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "# nltk.download('wordnet')\n",
        "print(lemmatizer.lemmatize(\"better\", 'a'))\n",
        "print(lemmatizer.lemmatize(\"going\", 'v'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axptQLOl5j2x",
        "outputId": "e51d9e39-78c9-45d2-9c5c-66f0d7a20380"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "good\n",
            "go\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "from nltk import word_tokenize\n",
        "\n",
        "# nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "text = \"GeeksforGeeks is a Computer Science platform.\"\n",
        "tokenized_text = word_tokenize(text)\n",
        "tags = tokens_tag = pos_tag(tokenized_text)\n",
        "tags\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6B5CoABs9TWb",
        "outputId": "e8515127-311c-4d67-ef79-a6613abe59e1"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('GeeksforGeeks', 'NNP'),\n",
              " ('is', 'VBZ'),\n",
              " ('a', 'DT'),\n",
              " ('Computer', 'NNP'),\n",
              " ('Science', 'NNP'),\n",
              " ('platform', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# Function to map NLTK POS tags to WordNet POS tags\n",
        "def get_wordnet_pos(nltk_pos_tag):\n",
        "    \"\"\"Map NLTK POS tag to WordNet POS tag\"\"\"\n",
        "    if nltk_pos_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif nltk_pos_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif nltk_pos_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif nltk_pos_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "\n",
        "# Sample text\n",
        "text = \"The striped bats are hanging on their feet for best\"\n",
        "\n",
        "# Tokenize and POS tag the text\n",
        "tokens = nltk.word_tokenize(text)\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "# Initialize the lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Lemmatize each word with its POS tag\n",
        "lemmatized_sentence = []\n",
        "for token, pos_tag in pos_tags:\n",
        "    wordnet_pos = get_wordnet_pos(pos_tag)\n",
        "    lemmatized_token = lemmatizer.lemmatize(token, pos=wordnet_pos)\n",
        "    lemmatized_sentence.append(lemmatized_token)\n",
        "\n",
        "# Print original and lemmatized sentence\n",
        "print(\"Original Sentence:\", text)\n",
        "print(\"Lemmatized Sentence:\", \" \".join(lemmatized_sentence))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g53NTkJf_YCj",
        "outputId": "abb95646-4686-405d-dfe7-bb2a976800b9"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sentence: The striped bats are hanging on their feet for best\n",
            "Lemmatized Sentence: The striped bat be hang on their foot for best\n"
          ]
        }
      ]
    }
  ]
}